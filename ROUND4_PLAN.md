# 第4轮训练计划

## 📊 前3轮总结

| 轮次 | 配置亮点 | 最佳成绩 | 问题 |
|------|---------|---------|------|
| Round 1 | 种群50, [64,32], 精英10% | 43.29 | 早熟收敛 |
| Round 2 | 种群100, 自适应GA | 8.24 | 自适应失败 |
| Round 3 | 种群150, [128,64,32], 固定0.2变异 | **105.78** | 学习慢但稳定 |

## 🎯 第4轮策略

### 核心思路：增强初期探索 + 引入多样性注入

既然大网络有效但进展慢，我们需要：
1. **更激进的初期探索** - 帮助快速找到有效区域
2. **周期性多样性注入** - 防止陷入局部最优
3. **分阶段训练** - 粗调 → 精调

### 配置方案 A：超高初始变异 + 逐步衰减

```python
POPULATION_SIZE = 200        # 进一步增大
GENERATIONS = 400            # 更多代数
HIDDEN_LAYERS = [256, 128, 64]  # 更大网络
ELITE_RATIO = 0.01           # 仅2个精英

# 变异策略：手动分阶段
# Gen 0-100:   mutation_rate = 0.30 (高探索)
# Gen 100-200: mutation_rate = 0.20 (平衡)
# Gen 200-400: mutation_rate = 0.15 (收敛)
```

### 配置方案 B：多样性注入机制

```python
POPULATION_SIZE = 150
GENERATIONS = 400
HIDDEN_LAYERS = [128, 64, 32]  # 保持第3轮的网络
MUTATION_RATE = 0.20

# 每50代注入10个随机个体，替换最差的
DIVERSITY_INJECTION = True
INJECTION_FREQUENCY = 50
INJECTION_SIZE = 10
```

### 配置方案 C：混合策略（推荐⭐）

结合A和B的优点：

```python
POPULATION_SIZE = 180        # 折中
GENERATIONS = 400
HIDDEN_LAYERS = [256, 128, 64]  # 大网络
ELITE_RATIO = 0.015          # 约3个精英
MUTATION_RATE = 0.25         # 保持高探索
MUTATION_SCALE = 0.5         # 大变异幅度

# 多样性注入
每75代注入15个高质量随机个体
```

## 🔧 推荐配置（第4轮）

基于第3轮的成功经验，采用**渐进式改进**：

```python
# config.py 修改

POPULATION_SIZE = 180        # 150 → 180 (↑20%)
GENERATIONS = 400            # 300 → 400 (有更多机会突破)
MUTATION_RATE = 0.25         # 0.20 → 0.25 (更强探索)
MUTATION_SCALE = 0.5         # 0.40 → 0.50 (更大跳跃)
CROSSOVER_RATE = 0.75        # 0.80 → 0.75 (减少，增加独立个体)
ELITE_RATIO = 0.015          # 0.02 → 0.015 (约3个精英)
TOURNAMENT_SIZE = 5          # 7 → 5 (降低选择压力)

HIDDEN_LAYERS = [256, 128, 64]  # [128,64,32] → [256,128,64] 
                                # 参数数量: 13K → ~36K
```

### 为什么这样调整？

1. **网络容量翻倍** (13K → 36K参数)
   ```
   理由: 105分说明网络可以学，但容量可能还不够
   更大网络 = 更强的表达能力
   ```

2. **提高变异率** (0.20 → 0.25)
   ```
   理由: 第3轮22次突破说明探索有效
   更高变异 = 更快发现新策略
   ```

3. **增大变异幅度** (0.40 → 0.50)
   ```
   理由: 当前在90-105徘徊，需要更大跳跃
   大幅度 = 更容易跳出局部最优
   ```

4. **更多代数** (300 → 400)
   ```
   理由: 第3轮到第290代还在进步
   更多代数 = 更充分的进化
   ```

5. **轻微降低选择压力** (tournament 7 → 5)
   ```
   理由: 给中等个体更多机会
   平衡 = 避免过快收敛
   ```

## 📈 预期效果

| 阶段 | 代数 | 预期最佳 | 行为 |
|------|------|---------|------|
| 初探索 | 0-100 | 0-80 | 快速探索，发现基本策略 |
| 突破期 | 100-200 | 80-150 | 突破局部最优，找到更好策略 |
| 优化期 | 200-300 | 150-200 | 优化步态，稳定提升 |
| 精炼期 | 300-400 | 200+ | 达到较好的行走效果 |

**目标**: 最佳适应度 ≥ 200 分

## ⏱️ 时间预估

```
单代时间: ~25秒 (种群180, 大网络)
400代总时间: ~166分钟 (约2.5-3小时)
```

## 🚨 风险与备选

### 风险1: 训练时间太长
**对策**: 可以先训练200代看效果，再决定是否继续

### 风险2: 大网络过拟合
**对策**: 如果后期适应度下降，停止训练

### 风险3: 仍然不够好
**备选方案**:
- 尝试NEAT算法（进化网络拓扑）
- 尝试CMA-ES（更高级的进化算法）
- 换更简单的环境验证算法
- 考虑使用梯度方法（PPO/SAC）作为对比

## 📝 训练检查点

每100代检查：
- ✅ 有NEW RECORD出现 → 继续
- ✅ 平均适应度上升 → 继续
- ⚠️ 超过100代无突破 → 考虑调整
- ❌ 适应度下降 → 停止

## 🎓 关键见解

1. **网络容量很重要**: 3.8K → 13.7K 参数带来了2.4倍提升
2. **持续探索有效**: 固定0.2变异率比自适应更好
3. **大种群必要**: 150个个体才有足够多样性
4. **需要耐心**: 最佳结果出现在第290代

## 💡 如果第4轮成功

成功标志：最佳 ≥ 200 分

下一步可以：
1. 尝试Humanoid-v4（3D环境）
2. 尝试其他进化算法对比
3. 撰写完整的实验报告
4. 可视化进化过程

## 📚 准备工作

1. ✅ Git保存了第3轮代码
2. ✅ 删除了模型和视频
3. ⏭️ 修改config.py为新配置
4. ⏭️ 启动第4轮训练
5. ⏭️ 监控训练进度

---

**准备好了吗？让我们开始第4轮！** 🚀

