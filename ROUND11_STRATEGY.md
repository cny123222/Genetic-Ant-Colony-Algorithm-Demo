# Round 11-15: 多次运行 + 更大网络

## 策略：双重方案

### 方案A: 跑5次Round 7配置（碰运气）
**理由**：
- Round 7得到173.26（很好）
- Round 10得到157.53（差运气）
- 说明相同参数会有不同结果
- 跑5次，取最好的

**配置**：
```python
POPULATION_SIZE = 180
GENERATIONS = 600
MUTATION_RATE = 0.25
HIDDEN_LAYERS = [256, 128, 64]  # 47K参数
```

**预期范围**：150 ~ 180（取决于运气）
**目标**：至少有1次能达到170+

### 方案B: 尝试更大网络
**假设**：也许47K参数不够

#### 配置1: 中等增大
```python
HIDDEN_LAYERS = [384, 256, 128]  # ~73K参数
```

#### 配置2: 显著增大
```python
HIDDEN_LAYERS = [512, 256, 128]  # ~94K参数
```

#### 配置3: 非常大（对比实验）
```python
HIDDEN_LAYERS = [512, 512, 256]  # ~170K参数
```

**风险**：
- 参数空间更大，可能更难优化
- 需要更多代数
- 可能反而更差

## 网上的成功案例

### BipedalWalker-v3的基准分数

**官方解决标准**：
- 平均奖励 > 300（连续100次评估）
- 最高分理论值：~340

**各种方法的典型成绩**：
1. **PPO（Proximal Policy Optimization）**：
   - 训练好的：280-320
   - 网络：[256, 256] 或 [400, 300]
   - 训练时间：1-2M steps

2. **SAC（Soft Actor-Critic）**：
   - 训练好的：300-330
   - 网络：[256, 256]
   - 训练时间：500K-1M steps

3. **DDPG/TD3**：
   - 训练好的：250-300
   - 网络：[400, 300]

4. **遗传算法/进化策略**：
   - 文献报告：100-250
   - 网络：通常[128, 64]到[256, 128]
   - 很少见到>250的

### 为什么GA难达到300+？

**根本原因**：
1. **样本效率低**：
   - PPO用1M steps ≈ 1000 episodes
   - 我们的GA：600代 × 180个体 = 108K episodes
   - **我们用了100倍样本！**

2. **无梯度信息**：
   - RL方法知道"怎么改进"
   - GA只知道"好还是不好"
   - 盲目搜索效率低

3. **局部最优**：
   - 47K维空间有无数局部最优
   - GA容易卡住
   - RL有价值函数指导

### 网络大小的典型配置

| 方法 | 网络大小 | 参数量 | 成绩 |
|------|---------|--------|------|
| PPO | [256, 256] | ~133K | 300+ |
| SAC | [256, 256] | ~133K | 300+ |
| TD3 | [400, 300] | ~250K | 280+ |
| **GA（我们）** | **[256, 128, 64]** | **47K** | **173** |
| GA（文献） | [128, 64] | ~13K | 150-200 |

**观察**：
- 我们的47K在GA中已经算大的
- RL方法用更大网络（100K+）
- 但RL方法本身效率高

## 实验计划

### 阶段1: 5次Round 7（2-3天）

**Round 11-15**：完全相同配置，跑5次
```python
# Round 7配置
POPULATION_SIZE = 180
GENERATIONS = 600
MUTATION_RATE = 0.25
HIDDEN_LAYERS = [256, 128, 64]
```

**并行运行**：
- 如果有多核CPU，可以同时跑2-3个
- 每个约3小时
- 总时间：如果串行15小时，并行5-8小时

**预期分布**：
```
最好情况：180+（小概率）
中位数：165
最差情况：155
```

### 阶段2: 更大网络（如果阶段1不满意）

**Round 16**: 尝试 [512, 256, 128]
```python
POPULATION_SIZE = 180
GENERATIONS = 800  # 更大网络需要更多时间
MUTATION_RATE = 0.20  # 更多参数，降低变异
HIDDEN_LAYERS = [512, 256, 128]  # 94K参数
```

**理由**：
- 2倍参数量（47K → 94K）
- 接近RL方法的网络大小
- 但可能需要降低变异率（参数更多）

## 成功标准

### 目标分数
- 理想：190+（接近200）
- 满意：180+（比Round 7好）
- 可接受：175+（与Round 7相当）

### 决策点
**如果5次Round 7最好<175**：
→ 说明47K参数就是瓶颈，试更大网络

**如果5次Round 7最好>180**：
→ 成功！参数合理，只是需要好运气

**如果更大网络也<180**：
→ GA方法本身的局限，该换RL了

## 时间估算

```
方案A（5次Round 7）：
- 串行：5 × 3小时 = 15小时
- 并行（3个）：6小时

方案B（更大网络）：
- 1次：4-5小时

总计：如果需要，1-2天完成所有实验
```

## 备选方案：直接用PPO

**如果时间紧迫**：
```python
from stable_baselines3 import PPO

model = PPO(
    "MlpPolicy", 
    "BipedalWalker-v3",
    policy_kwargs=dict(net_arch=[256, 256]),
    learning_rate=3e-4,
    n_steps=2048,
    batch_size=64,
)
model.learn(total_timesteps=2000000)
model.save("ppo_bipedal")
```

**预期**：
- 训练时间：2-3小时
- 成绩：250-300+
- 但这是RL方法，不是GA

## 推荐执行顺序

1. **立即开始**：跑3个Round 7（并行）
2. **同时启动**：1个更大网络实验
3. **明天查看**：对比结果
4. **如果都<180**：考虑换PPO或接受现状

## 现实预期

**老实说**：
- GA在BipedalWalker上达到300+极其困难
- 文献中GA很少报告>250
- 我们的173已经是不错的成绩
- 180-200是现实的上限目标

**如果目标是>250**：
- 强烈建议换用PPO/SAC
- GA不是最佳方法
- 这不是参数问题，是方法问题

