# 训练改进说明

## 第一次训练问题分析（已完成）

### 问题诊断

| 指标 | 第一次训练 | 问题 |
|------|-----------|------|
| 种群大小 | 50 | ❌ 太小，多样性不足 |
| 变异率 | 0.15 | ❌ 太低，探索不足 |
| 变异幅度 | 0.3 | ❌ 太小，难以逃离局部最优 |
| 精英比例 | 0.1 (5个) | ❌ 太高，过度复制优秀个体 |
| 算法类型 | 标准GA | ❌ 无法自适应调整 |

### 表现结果

- **历史最佳**: 43.29 (第36代)
- **后续464代**: 再无突破，适应度在-1到0徘徊
- **最终表现**: reward = -15.6 （退化）
- **问题**: **早熟收敛** (Premature Convergence)

## 第二次训练改进方案

### 参数调整

| 参数 | 第一次 → 第二次 | 原因 |
|------|---------------|------|
| 种群大小 | 50 → **100** | ✓ 增加种群多样性，提供更多探索空间 |
| 变异率 | 0.15 → **0.25** | ✓ 增强探索能力，更容易逃离局部最优 |
| 变异幅度 | 0.3 → **0.5** | ✓ 扩大探索范围 |
| 精英比例 | 0.1 (5) → **0.05 (5)** | ✓ 减少优秀基因的过度复制 |
| 交叉率 | 0.8 → **0.7** | ✓ 增加独立个体数量 |
| 锦标赛 | 3 → **5** | ✓ 适度增加选择压力 |
| 算法类型 | 标准 → **自适应** | ✓ 自动调整变异率，停滞时增加探索 |

### 改进原理

#### 1. 增大种群规模 (50 → 100)
```
更多个体 → 更多基因组合 → 更高多样性 → 更难早熟
```

#### 2. 提高变异率 (0.15 → 0.25)
```
每代约25%的参数变异 → 持续引入新基因 → 防止多样性丧失
```

#### 3. 增大变异幅度 (0.3 → 0.5)
```
更大的参数扰动 → 可以跳得更远 → 更容易逃离局部最优
```

#### 4. 降低精英比例 (10% → 5%)
```
减少精英个体 → 减少优秀基因的垄断 → 给其他个体更多机会
```

#### 5. 使用自适应遗传算法
```
检测停滞 → 自动增加变异率 → 重新激活探索
停滞 < 5代：逐步降低变异率 (收敛)
停滞 ≥ 5代：增加变异率和幅度 (探索)
```

### 预期效果

| 阶段 | 代数 | 预期适应度 | 行为 |
|------|------|-----------|------|
| 探索期 | 0-100 | -50 到 50 | 大量尝试，找到有效策略 |
| 学习期 | 100-200 | 50 到 150 | 稳定前进，学会基本行走 |
| 优化期 | 200-400 | 150 到 250 | 优化步态，提高效率 |
| 精炼期 | 400-500 | 250+ | 流畅行走，可能完成地形 |

### 成功标志

✅ **避免早熟**：最佳适应度持续增长，不出现长期停滞
✅ **多样性保持**：平均适应度稳步提升
✅ **持续改进**：100代后仍能发现新记录
✅ **最终性能**：适应度达到 200+ （BipedalWalker算很好）

### 风险与对策

**风险1：训练时间增加**
- 原因：种群×2，每代时间翻倍
- 预计：约60-90分钟（原来30分钟）
- 对策：可以接受，性能提升更重要

**风险2：收敛可能变慢**
- 原因：高变异率可能延缓收敛
- 对策：自适应算法会在需要时降低变异率

**风险3：计算资源需求**
- 原因：种群翻倍
- 对策：后台运行，不影响其他工作

## 监控要点

### 关注指标

1. **NEW RECORD出现频率**
   - 理想：前200代频繁出现
   - 警告：超过50代无新记录

2. **变异率变化** (MR)
   - 停滞时应该上升
   - 进步时应该下降

3. **平均适应度趋势**
   - 应该稳步上升
   - 不应该大幅波动

4. **标准差**
   - 保持在30-50范围
   - 太小(<20)表示多样性丧失

### 实时监控命令

```bash
# 查看最新进度
tail -20 training_log_*.txt

# 查看新记录
grep "NEW RECORD" training_log_*.txt | tail -10

# 查看变异率变化
grep "MR:" training_log_*.txt | tail -20
```

## 如果仍然不行

### 进一步调整方案

1. **继续增大种群** (100 → 150)
2. **使用更大的网络** [64,32] → [128,64,32]
3. **引入diversity injection** (周期性注入随机个体)
4. **使用岛屿模型** (多个独立种群，定期交换)
5. **尝试不同的选择策略** (排名选择、轮盘赌)

### 环境替换

如果BipedalWalker太难，可以尝试：
```python
ENV_NAME = 'LunarLanderContinuous-v2'  # 更容易
```

## 理论基础

### 遗传算法的探索-利用权衡

```
探索 (Exploration)：发现新区域
  - 高变异率
  - 大变异幅度
  - 低精英比例

利用 (Exploitation)：优化已知区域  
  - 低变异率
  - 小变异幅度
  - 高精英比例
```

**第一次失败原因**：过早进入利用阶段，丢失探索能力

**第二次策略**：加强探索，延迟利用，使用自适应平衡

### 多样性维持策略

1. **大种群** - 物理多样性
2. **高变异** - 基因多样性  
3. **低精英** - 机会多样性
4. **自适应** - 动态多样性

## 参考

- Holland, J. H. (1992). Genetic Algorithms. Scientific American.
- Eiben, Á. E., & Smith, J. E. (2015). Introduction to Evolutionary Computing.
- DeJong, K. A. (2006). Evolutionary Computation: A Unified Approach.

---

**开始时间**: 2025-11-26 19:56
**预计完成**: 2025-11-26 21:00-21:30
**目标**: 避免早熟，达到适应度 200+

